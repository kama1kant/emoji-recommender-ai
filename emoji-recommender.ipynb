{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9dc82e-35b9-479b-8d47-819af95929f0",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df580ed9-df47-4905-ab13-00e5f906ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c65f31-6f20-46a2-9c4f-8b74cab3e890",
   "metadata": {},
   "source": [
    "### Build required functions inside a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2affc162-bb3d-4b2a-878a-d8d2332b3351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmojiRecommender:\n",
    "    def __init__(self):\n",
    "        self.getModel()\n",
    "    \n",
    "    def getModel(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "        self.model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "    \n",
    "    def getMeanTokensSentence(self, sentence):\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub('[^a-z]+', ' ', sentence)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = word_tokenize(sentence)\n",
    "        sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "        sentence = ' '.join(sentence)\n",
    "        return obj.getMeanTokens([sentence])\n",
    "        \n",
    "    def getMeanTokensCsv(self):\n",
    "        self.emoji_df = pd.read_csv(\"data/emoji-data.csv\")\n",
    "        self.all_emoji_df = self.emoji_df\n",
    "        self.emoji_df = self.emoji_df[1800:2000]\n",
    "        self.emoji_df = self.emoji_df.reset_index(drop=True)\n",
    "        return self.getMeanTokens(self.emoji_df['description'])\n",
    "    \n",
    "    def getMeanTokens(self, sentences):\n",
    "        self.getTokens(sentences)\n",
    "        self.getEmbedding()\n",
    "        return self.getMeanValue()\n",
    "    \n",
    "    def getTokens(self, sentences):\n",
    "        self.tokens = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "        for sentence in sentences:\n",
    "            new_tokens = self.tokenizer.encode_plus(sentence, max_length=128,\n",
    "                                               truncation=True, padding='max_length',\n",
    "                                               return_tensors='pt')\n",
    "            self.tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "            self.tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
    "\n",
    "        self.tokens['input_ids'] = torch.stack(self.tokens['input_ids'])\n",
    "        self.tokens['attention_mask'] = torch.stack(self.tokens['attention_mask'])\n",
    "    \n",
    "    def getEmbedding(self):\n",
    "        outputs = self.model(**self.tokens)\n",
    "        self.embeddings = outputs.last_hidden_state\n",
    "    \n",
    "    def getMeanValue(self):\n",
    "        attention_mask = self.tokens['attention_mask']\n",
    "        mask = attention_mask.unsqueeze(-1).expand(self.embeddings.size()).float()\n",
    "        masked_embeddings = self.embeddings * mask\n",
    "        summed = torch.sum(masked_embeddings, 1)\n",
    "        summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "        self.mean_pooled = summed / summed_mask\n",
    "        self.mean_pooled = self.mean_pooled.detach().numpy()\n",
    "        return self.mean_pooled\n",
    "\n",
    "    def getSimilarity(self, sentence_tokens, mean_tokens):\n",
    "        similarity = cosine_similarity([sentence_tokens],mean_tokens)\n",
    "        return similarity\n",
    "        \n",
    "    def build_emoji_csv(self):\n",
    "        df = pd.read_csv(\"data/raw-emoji-data.csv\", usecols=[1, 3], squeeze = True, header = None)\n",
    "        df = df.dropna()\n",
    "        df = df.iloc[1:, :]\n",
    "        self.df_to_csv(df)\n",
    "    \n",
    "    def df_to_csv(self, df):\n",
    "        df = pd.DataFrame({'emoji': df[1], 'description': df[3]})\n",
    "        df.to_csv(\"data/emoji-data.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b019ad-c2ef-4948-b7c8-2beb0913a6d0",
   "metadata": {},
   "source": [
    "### Initialize class object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ced9bd8-d150-4689-99c8-4aef81e710f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = EmojiRecommender()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792c43df-c765-4618-86b5-1c1e6fc504e2",
   "metadata": {},
   "source": [
    "### Read raw emoji data from \"raw-emoji-data.csv\", clean it & then store in \"emoji-data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8c82a38-9f4a-4b73-ad43-260b42034523",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.build_emoji_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6a37d3-bc9a-4cce-9b37-6992fbfd4aaf",
   "metadata": {},
   "source": [
    "### Test: Read top 200 emojis, process it & find top five recommended emojis for the example sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e7baee5-f0be-439b-872e-0485306ee46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 768)\n"
     ]
    }
   ],
   "source": [
    "mean_tokens = obj.getMeanTokensCsv()\n",
    "print(mean_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e122c46a-cbb5-4234-909b-6333d05e9434",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Will you go on a date with me?\"\n",
    "sentence_token = obj.getMeanTokensSentence(sentence)\n",
    "similarity = obj.getSimilarity(sentence_token[0], mean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a12cb1e0-b51a-484c-b531-44aa6461b9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 ðŸ¥³ partying face\n",
      "166 ðŸ¤Ÿ love you gesture\n",
      "119 ðŸ’‹ kiss mark\n",
      "189 ðŸ¤³ selfie\n",
      "120 ðŸ’Œ love letter\n"
     ]
    }
   ],
   "source": [
    "indices = (-similarity[0]).argsort()[:5]\n",
    "for i in indices:\n",
    "    print(i, obj.emoji_df['emoji'][i], obj.emoji_df['description'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa2b703-3ccf-4748-88cd-9d9d69abaff6",
   "metadata": {},
   "source": [
    "### Loop through \"emoji-data.csv\" & store the mean tokens in a .pt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0161ebb-78dd-4228-90ff-e5a78fe7ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code\n",
    "# torch.save(mean_tokens, 'checkpoint/token-all.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0561d954-d230-4c96-a83c-0ec8e64f6fe5",
   "metadata": {},
   "source": [
    "### 1. Build a function to load mean tokens for all emojis\n",
    "### 2. Find recommended emojis based on cosine similarity between mean tokens of text & emoji descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c59faadb-3db2-4704-b64a-0408f5bf806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_emoji(sentence):\n",
    "    name = 'token-all.pt'\n",
    "    all_tokens = torch.load('checkpoint/'+name)\n",
    "    sentence_token = obj.getMeanTokensSentence(sentence)\n",
    "    similarity = obj.getSimilarity(sentence_token[0], all_tokens)\n",
    "    indices = (-similarity[0]).argsort()[:5]\n",
    "    emoji_df = pd.read_csv(\"data/emoji-data.csv\")\n",
    "    for j in indices:\n",
    "        print(emoji_df['emoji'][j], emoji_df['description'][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63a89da0-5e10-4a59-991e-5a6b4797a0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ movie camera\n",
      "ðŸŽ¦ cinema\n",
      "ðŸ“½ film projector\n",
      "ðŸ“€ dvd\n",
      "ðŸŽž film frames\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I am going to the movies\"\n",
    "recommend_emoji(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e19a9-0556-426f-aedc-f734d78ad72d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81c1be6-af5e-4a2e-aaf7-759d6f4f320f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
